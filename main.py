import os
import json
import time
import datetime
import requests
from bs4 import BeautifulSoup

# LangChain å¯¼å…¥ (æŒ‰éœ€åŠ è½½ï¼Œé¿å…æœªå®‰è£…æ—¶æŠ¥é”™)
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

# --- é…ç½®éƒ¨åˆ† ---
CONFIG = {
    "urls": {
        "General": "https://github.com/trending",
        "Python": "https://github.com/trending/python",
        "Go": "https://github.com/trending/go"
    },
    "history_file": "data/history.json",
    "archive_dir": "archives",
    "readme_file": "README.md",
    "enable_llm": os.getenv("ENABLE_LLM", "false").lower() == "true",
    "openai_api_key": os.getenv("OPENAI_API_KEY"),
    "openai_base_url": os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
}

# --- æ•°æ®ç»“æ„ ---
# ç®€å•çš„é¡¹ç›®ç±»
class Repo:
    def __init__(self, owner, name, description, lang, stars, url):
        self.owner = owner
        self.name = name
        self.full_name = f"{owner}/{name}"
        self.description = description
        self.lang = lang
        self.stars = stars
        self.url = url
        self.ai_summary = ""
        self.is_new = True # é»˜è®¤ä¸ºæ–°é¡¹ç›®

    def to_dict(self):
        return {
            "full_name": self.full_name,
            "description": self.description,
            "ai_summary": self.ai_summary,
            "url": self.url
        }

# --- æ ¸å¿ƒåŠŸèƒ½ ---

def load_history():
    """åŠ è½½å†å²è®°å½•ï¼Œç”¨äºå»é‡å’Œé¿å…é‡å¤ç”Ÿæˆæ‘˜è¦"""
    if not os.path.exists(CONFIG["history_file"]):
        return {}
    with open(CONFIG["history_file"], 'r', encoding='utf-8') as f:
        try:
            return json.load(f)
        except:
            return {}

def save_history(history):
    """ä¿å­˜å†å²è®°å½•"""
    os.makedirs(os.path.dirname(CONFIG["history_file"]), exist_ok=True)
    with open(CONFIG["history_file"], 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def scrape_github_trending(url):
    """æŠ“å– GitHub Trending é¡µé¢"""
    print(f"æ­£åœ¨æŠ“å–: {url}")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        if resp.status_code != 200:
            print(f"æŠ“å–å¤±è´¥: {resp.status_code}")
            return []
    except Exception as e:
        print(f"è¯·æ±‚å¼‚å¸¸: {e}")
        return []

    soup = BeautifulSoup(resp.text, 'html.parser')
    repos = []
    
    for article in soup.select('article.Box-row'):
        try:
            h2 = article.select_one('h2 a')
            if not h2: continue
            
            href = h2['href'] # /owner/repo
            parts = href.strip('/').split('/')
            if len(parts) < 2: continue
            
            owner, name = parts[0], parts[1]
            repo_url = f"https://github.com{href}"
            
            desc_tag = article.select_one('p.col-9')
            description = desc_tag.text.strip() if desc_tag else "æ— æè¿°"
            
            lang_tag = article.select_one('span[itemprop="programmingLanguage"]')
            lang = lang_tag.text.strip() if lang_tag else "Unknown"
            
            # è·å– Stars (é€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªå«æœ‰é“¾æ¥çš„ text)
            # ç®€åŒ–å¤„ç†ï¼Œä¸ä¸€å®šéå¸¸ç²¾ç¡®ï¼Œä½†é€šå¸¸æœ‰æ•ˆ
            stats_div = article.select_one('div.f6.color-fg-muted.mt-2')
            stars = "N/A"
            if stats_div:
                star_link = stats_div.select_one('a[href$="/stargazers"]')
                if star_link:
                    stars = star_link.text.strip()

            repos.append(Repo(owner, name, description, lang, stars, repo_url))
        except Exception as e:
            print(f"è§£æå•ä¸ªé¡¹ç›®å‡ºé”™: {e}")
            continue
            
    return repos

def generate_ai_summary(repo: Repo):
    """ä½¿ç”¨ LangChain ç”Ÿæˆç®€æŠ¥"""
    if not LANGCHAIN_AVAILABLE or not CONFIG["enable_llm"] or not CONFIG["openai_api_key"]:
        return "LLM æœªå¯ç”¨æˆ–æœªé…ç½® Keyã€‚"

    print(f"æ­£åœ¨ä¸º {repo.full_name} ç”Ÿæˆ AI ç®€æŠ¥...")
    
    try:
        llm = ChatOpenAI(
            api_key=CONFIG["openai_api_key"],
            base_url=CONFIG["openai_base_url"],
            model="gpt-3.5-turbo", # æˆ–è€… gpt-4
            temperature=0.3
        )

        prompt = ChatPromptTemplate.from_template(
            "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡ç®€è¦æ€»ç»“ä»¥ä¸‹ GitHub é¡¹ç›®çš„åŠŸèƒ½å’Œäº®ç‚¹ã€‚\n"
            "é¡¹ç›®åç§°: {name}\n"
            "è¯­è¨€: {lang}\n"
            "åŸå§‹æè¿°: {desc}\n"
            "è¯·ç”¨ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸è¦åºŸè¯ã€‚"
        )

        chain = prompt | llm | StrOutputParser()
        summary = chain.invoke({"name": repo.name, "lang": repo.lang, "desc": repo.description})
        return summary
    except Exception as e:
        print(f"AI ç”Ÿæˆå¤±è´¥: {e}")
        return "AI ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸå§‹æè¿°ã€‚"

def update_readme_index(archive_files):
    """æ›´æ–°ä¸» README ç´¢å¼•"""
    header = """# ğŸ“ˆ GitHub Trending æ¯æ—¥è¿½è¸ª

è¿™ä¸ªä»“åº“é€šè¿‡ Github Action æ¯å¤©è‡ªåŠ¨æŠ“å– GitHub Trending çƒ­ç‚¹ã€‚
åŒ…å« **General**, **Python**, **Go** ä¸‰ä¸ªåˆ†ç±»ã€‚

- **è‡ªåŠ¨å»é‡**: å†å²è®°å½•ä¸­å·²å­˜åœ¨çš„é¡¹ç›®ä¸ä¼šé‡å¤è¿›è¡Œ AI åˆ†æã€‚
- **AI ç®€æŠ¥**: ä½¿ç”¨ LangChain ç”Ÿæˆé¡¹ç›®ä¸­æ–‡æ‘˜è¦ï¼ˆå¦‚æœåœ¨ Action ä¸­å¯ç”¨ï¼‰ã€‚

## ğŸ—‚ å†å²å½’æ¡£ (Archives)

| æ—¥æœŸ (Date) | é“¾æ¥ (Link) |
|---|---|
"""
    # æŒ‰æ–‡ä»¶åå€’åºï¼ˆæ—¥æœŸæœ€æ–°çš„åœ¨å‰ï¼‰
    archive_files.sort(reverse=True)
    
    content = header
    for f in archive_files:
        if not f.endswith(".md"): continue
        date_str = f.replace(".md", "")
        content += f"| {date_str} | [æŸ¥çœ‹æ—¥æŠ¥](./{CONFIG['archive_dir']}/{f}) |\n"

    with open(CONFIG['readme_file'], 'w', encoding='utf-8') as f:
        f.write(content)

def main():
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")
    history = load_history()
    
    all_content_md = f"# ğŸš€ GitHub Trending {today_str}\n\n"
    
    has_new_content = False

    for category, url in CONFIG["urls"].items():
        print(f"\n--- å¤„ç†åˆ†ç±»: {category} ---")
        repos = scrape_github_trending(url)
        
        category_md = f"## {category} Trending\n\n"
        category_md += "| é¡¹ç›® | ç®€ä»‹ (AI/Original) | Stars | çŠ¶æ€ |\n"
        category_md += "|---|---|---|---|\n"
        
        count = 0
        for repo in repos:
            # å»é‡é€»è¾‘
            # å¦‚æœå†å²è®°å½•é‡Œæœ‰ï¼Œæˆ‘ä»¬è®¤ä¸ºæ˜¯"æ—§é¡¹ç›®"ï¼Œä¸å†ç”Ÿæˆ AI æ‘˜è¦ï¼Œä½†ä¾ç„¶å¯ä»¥åˆ—åœ¨ä»Šæ—¥æ¦œå•é‡Œ
            # å¦‚æœç”¨æˆ·å¸Œæœ›"å®Œå…¨ä¸ä¿å­˜"é‡å¤é¡¹ç›®ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼š
            # if repo.full_name in history: continue 

            is_historied = repo.full_name in history
            repo.is_new = not is_historied
            
            summary = repo.description
            
            if repo.is_new:
                # æ˜¯æ–°é¡¹ç›®ï¼Œä¸”å¼€å¯äº† LLMï¼Œåˆ™ç”Ÿæˆæ‘˜è¦
                if CONFIG["enable_llm"]:
                    ai_sum = generate_ai_summary(repo)
                    repo.ai_summary = ai_sum
                    summary = f"ğŸ¤– **AI**: {ai_sum}"
                    # è®°å½•åˆ°å†å²
                    history[repo.full_name] = repo.to_dict()
                    has_new_content = True
                else:
                    # æ²¡å¼€ AIï¼Œè®°å½•åŸå§‹ä¿¡æ¯åˆ°å†å²é˜²æ­¢ä¸‹æ¬¡è¢«å½“åšå…¨æ–°çš„
                    history[repo.full_name] = repo.to_dict()
                    has_new_content = True
            else:
                # æ˜¯æ—§é¡¹ç›®ï¼Œå°è¯•ä»å†å²è¯»å– AI æ‘˜è¦
                cached = history.get(repo.full_name, {})
                if cached.get("ai_summary"):
                    summary = f"ğŸ¤– **AI (Cached)**: {cached['ai_summary']}"
            
            status_icon = "ğŸ†•" if repo.is_new else "ğŸ”"
            
            # æ ¼å¼åŒ–è¡¨æ ¼è¡Œ (å¤„ç† Markdown ç ´åå­—ç¬¦)
            clean_desc = summary.replace("|", "\\|").replace("\n", " ")
            row = f"| [{repo.owner}/{repo.name}]({repo.url}) | {clean_desc} | {repo.stars} | {status_icon} |\n"
            category_md += row
            count += 1
            
        if count > 0:
            all_content_md += category_md + "\n"
        
        # ç¤¼è²Œæ€§å»¶è¿Ÿ
        time.sleep(2)

    # ä¿å­˜æ¯æ—¥å½’æ¡£
    os.makedirs(CONFIG["archive_dir"], exist_ok=True)
    daily_file_path = os.path.join(CONFIG["archive_dir"], f"{today_str}.md")
    
    with open(daily_file_path, 'w', encoding='utf-8') as f:
        f.write(all_content_md)
    print(f"å·²ç”Ÿæˆæ—¥æŠ¥: {daily_file_path}")

    # ä¿å­˜å†å²è®°å½•æ•°æ®åº“
    save_history(history)

    # æ›´æ–°æ€»ç›®å½•
    archives = os.listdir(CONFIG["archive_dir"])
    update_readme_index(archives)

if __name__ == "__main__":
    main()